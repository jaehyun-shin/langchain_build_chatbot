{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 구축\n",
    "## 개요\n",
    "\n",
    "LLM 기반 챗봇을 설계하고 구현하는 방법의 예를 살펴보겠습니다. 이 챗봇은 대화를 나누고 이전 상호 작용을 기억할 수 있습니다.\n",
    "우리가 구축한 이 챗봇은 언어 모델만 사용하여 대화를 나눕니다. 다음과 같은 몇 가지 다른 관련 개념을 찾을 수 있습니다.\n",
    "\n",
    "- [대화형 RAG](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/): 외부 데이터 소스를 통해 챗봇 경험을 활성화합니다.\n",
    "- [챗봇](https://python.langchain.com/v0.2/docs/tutorials/agents/): 조치를 취할 수 있는 챗봇 구축\n",
    "\n",
    "## 개념\n",
    "\n",
    "우리가 작업할 고급 구성 요소입니다.\n",
    "\n",
    "- [채팅 모델](https://python.langchain.com/v0.2/docs/concepts/#chat-models): 챗봇 인터페이스는 원시 텍스트가 아닌 메시지를 기반으로 하므로 텍스트 LLM보다는 채팅 모델에 가장 적합합니다.\n",
    "- [프롬프트 템플릿](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates): 기본 메시지, 사용자 입력, 채팅 기록 및 검색된 추가 컨텍스트를 결합하는 프롬프트를 조합하는 프로세스를 단순화 합니다.\n",
    "- [채팅 기록](https://python.langchain.com/v0.2/docs/concepts/#chat-history): 챗봇이 과거 상호작용을 기억하고 후속 질문에 응답할 때 이를 고려할 수 있습니다.\n",
    "- [LangSmith](https://python.langchain.com/v0.2/docs/concepts/#langsmith) 를 사용하여 응용 프로그램 디버깅 및 추적\n",
    "  \n",
    "위의 구성 요솔를 함께 사용하여 강력한 대화형 챗봇을 만드는 방법을 다룹니다.\n",
    "\n",
    "## 설치\n",
    "\n",
    "### Jupyther Notebook\n",
    "\n",
    "이 예제는 Jupyter Notebook을 사용하여 진행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (0.2.1)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (0.1.67)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (2.7.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 변수 설정 및 랭스미스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\") # true\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") # your langchain api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_LLM\")\n",
    "embedding_model_name = os.getenv(\"OPENAI_EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개발\n",
    "\n",
    "먼저 LLM 모델 인스턴스를 불러옵니다.\n",
    "\n",
    "모델에 Messages를 입력해주고 invoke하여 모델을 호출해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Jaehyun! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cff4f501-04dd-4d5f-8852-d5d274b37948-0', usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 생성\n",
    "model = ChatOpenAI(model=model_name)\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Jaehyun\")])\n",
    "\n",
    "# AIMessage(content='Hello Jaehyun! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-cff4f501-04dd-4d5f-8852-d5d274b37948-0', usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 자체에는 상태 개념이 없습니다. 후속 질문을 하는 경우 이전의 질문 내용을 LLM이 저장하고 있지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am sorry, but I do not have the ability to know your name.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 12, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b09691d3-8400-4e13-9301-9fc47f2375c7-0', usage_metadata={'input_tokens': 12, 'output_tokens': 16, 'total_tokens': 28})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "\n",
    "# AIMessage(content='I am sorry, but I do not have the ability to know your name.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 12, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b09691d3-8400-4e13-9301-9fc47f2375c7-0', usage_metadata={'input_tokens': 12, 'output_tokens': 16, 'total_tokens': 28})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangSmith 추적](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r) 예제를 살펴보면\n",
    "이전 대화가 맥락으로 바뀌지 않아 질문에 답할 수 없다는 것을 알 수 있습니다. 이 문제를 해결하려면 전체 대화 기록을 모델에게 전달해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Jaehyun.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 39, 'total_tokens': 46}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-796d88b4-7558-4e82-9c7d-37836d1ff9dc-0', usage_metadata={'input_tokens': 39, 'output_tokens': 7, 'total_tokens': 46})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Jaehyun\"),\n",
    "        AIMessage(content=\"Hello Jaehyun! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# AIMessage(content='Your name is Jaehyun.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 39, 'total_tokens': 46}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-796d88b4-7558-4e82-9c7d-37836d1ff9dc-0', usage_metadata={'input_tokens': 39, 'output_tokens': 7, 'total_tokens': 46})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 대화 식으로 구현하면 챗봇이 이전 대화 내용을 토대로 상호작용할 수 있습니다. 그렇다면 이를 가장 잘 구현하려면 어떻게 해야하는지 알아봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메시지내역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Message History 클래스를 사용하여 모델을 래핑하고 상태 저장으로 만들 수 있습니다. 이렇게 하면 모델의 입력과 출력을 추적하고 일부 데이터 저장소에 저장합니다. 그런 다음 향후 상호작용은 해당 메시지를 로드하고 입력의 일부로 체인에 전달합니다.\n",
    "\n",
    "#### langchain-community 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (0.2.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (0.2.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (0.1.67)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\users\\infocz\\desktop\\study\\ai\\llm\\langchain\\langchain_build_chatbot\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.6 langchain_community-0.2.1 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구현\n",
    "\n",
    "`langchain_community`를 설치하였으면 관련 클래스를 가져오고 모델을 래핑해 메시지 기록을 추가하는 체인을 만듭니다.\n",
    "\n",
    "여기서 중요한 점은 이 함수는 메시지 기록을 개체(session)를 가져와서 반환해야합니다. 별도의 대화를 구별하는 데 사용되며 새 체인을 호출할 때 구성의 일부로 전달되어야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "  if session_id not in store:\n",
    "    store[session_id] = ChatMessageHistory()\n",
    "  return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great to hear! What kind of work do you do at Infocz?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke([HumanMessage(content=\"Hi! I.m work in infocz\")], config=config)\n",
    "\n",
    "print(f\"{response.content}\") # That's great to hear! What kind of work do you do at Infocz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion. You mentioned that you work at Infocz in your previous message. Can you please clarify what kind of work you do at Infocz?\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Where do i work?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "# I apologize for the confusion. You mentioned that you work at Infocz in your previous message. Can you please clarify what kind of work you do at Infocz?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챗봇이 이전의 대화를 기억하고 있습니다. 다른 `session_id`를 참조하면 대화가 새로 시작됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a virtual assistant and do not have a physical location where I work. I exist in the digital world to assist with tasks and answer questions.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Where do i work?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "# I'm a virtual assistant and do not have a physical location where I work. I exist in the digital world to assist with tasks and answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿\n",
    "\n",
    "프롬프트 템플릿은 원시 사용자 정보를 LLM이 작업할 수 있는 포맷으로 변환하는데 도움이 됩니다.\n",
    "이 경우 원시 사용자 입력은 LLM에 전달하는 메시지일 뿐입니다.\n",
    "몇 가지 사용자 지정 지침이 포함된 시스템 메시지를 추가해 봅시다.\n",
    "\n",
    "시스템 메시지를 추가하기 위해 ChatPromptTemplate를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "      \"system\",\n",
    "      \"You are a helpful assistant. Answer all questions to the best of your ability\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MessagesPlaceholder(variable_name=\"messages\")`는 입력 유형을 약간 변형한 것입니다. 메시지 목록이 포함된 키(`messages`)가 있는 딕셔너리를 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jaehyun! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm Jaehyun\")]})\n",
    "\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까 전 메시지 기록 개체 ( `RunnableWithMessageHistory` )로 해당 체인을 래핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jaehyun! How can I assist you today?\n",
      "Your name is Jaehyun.\n"
     ]
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "\n",
    "config = {\"configurable\": { \"session_id\" : \"abc5\" }}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content=\"Hi! I'm Jaehyun\")],\n",
    "  config=config\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content=\"What's my name?\")],\n",
    "  config=config\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 복잡한 프롬프트 템플릿을 만들어 보겠습니다. 답변 내용을 특정 언어로 번역하여 반환하고자 합니다. 그러기 위해서는 시스템 메시지 내에 특정 언어를 변수로 설정해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 재현님! 만나서 반가워요. 어떻게 도와드릴까요?\n",
      "당신의 이름은 Jaehyun입니다.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Jaehyun\")], \"language\": \"korean\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what's my name?\")], \"language\": \"korean\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대화 기록 관리\n",
    "\n",
    "챗봇을 구축할 때 중요한 개념 중 하나는 대화 기록을 관리하는 방법입니다. 관리되지 않는 상태로 두면 메시지 목록이 무제한으로 증가하여 LLM의 컨텍스트 창을 오버플로, 즉 토큰 제한에 걸릴 수 있습니다. 따라서 전달하는 메시지 크기를 제한하는 단계를 추가하는 것이 중요합니다.\n",
    "\n",
    "**중요한 것은 프롬프트 템플릿을 사용하기 전에 이 작업을 수행하지만 메시지 기록에서 이전 메시지를 로드한 후에 수행하려고 한다는 것입니다.**\n",
    "\n",
    "키를 적절하게 수정하는 프롬프트 앞에 간단한 단계를 추가한 다음 메시지 기록 클래스에서 새 체인을 래핑하여 이 작업을 수행합니다.\n",
    "\n",
    "전달된 메시지를 수정할 함수를 정의합니다.\n",
    "\n",
    "가장 최근 메시지를 선택하다로고 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def filter_messages(messages, k=10):\n",
    "  return messages[-k:]\n",
    "\n",
    "chain = (\n",
    "  RunnablePassthrough.assign(messages= lambda x: filter_messages(x[\"messages\"]))\n",
    "  | prompt\n",
    "  | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 `prompt`의 `MessagesPlaceholder(variable_name=\"messages\")`를 봅시다. messages가 변수명으로 지정되어 있습니다.\n",
    "`RunnablePassthrough` 프롬프트 템플릿에 지정된 변수 즉 딕셔너리 키의 값을 컨트롤할 수 있습니다.\n",
    "`RunnablePassthrough.assign(messages=)`를 통해 messages 키의 메시지 목록 값을 가져와 `lambda x: filter_messages(x[\"messages\"])`함수의 매개변수로 넣어주고 반환된 값을 messages의 값으로 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10개 이상의 메시지 길이의 메시지 목록을 만들면 초기 메시지에서 이름에 대한 정보를 기억하지 못하는 상황을 볼 수 있습니다.\n",
    "그러나 마지막 10개 내에 있는 정보 ( 아이스크림 맛 ) 를 물어보면 여전히 기억하고 있습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't know your name.\n",
      "바닐라 아이스크림입니다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"Korean\",\n",
    "    }\n",
    ")\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my fav ice cream\")],\n",
    "        \"language\": \"Korean\",\n",
    "    }\n",
    ")\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 아까 처럼 메시지 기록에 해당 체인을 래핑해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have access to personal information.\n",
      "I'm sorry, I don't have access to that information.\n"
     ]
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"Korean\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what's my favorite ice cream?\")],\n",
    "        \"language\": \"Korean\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름을 물으면서 ai 답변하는 것까지 총 2개의 채팅이 신규 추가되었습니다. 따라서 가장 좋아하는 아이스크림 맛에 대한 채팅 기록이 최근 10개의 메시지 목록 내에 존재하지 않게 되면서 ai는 모른다고 답변을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍\n",
    "\n",
    "챗봇을 개발했습니다. 그러나 챗봇 애플리케이션에서 정말 중요한 UX 고려 사항 중 하나는 스트리밍입니다.\n",
    "LLM은 때때로 응답하는데 시간이 오래 걸릴 수 있으므로 사용자 경험을 개선하기 위해 대부분의 애플리케이션이 수행하는 한 가지는 각 토큰이 생성될 때 다시 스트리밍 하는 것 입니다.\n",
    "이를 통해 사용자는 진행 상황을 볼 수 있습니다.\n",
    "\n",
    "`.stream`을 사용하여 스트리밍 응답을 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|안|녕|하세요|,| 재|현|님|!| 물|론|이|죠|.| 여|기| 재|미|있|는| 농|담|이| 있|어|요|.\n",
      "\n",
      "|\"|왕|이| 꿈|을| 꾸|었|어|요|.| 어|떤| 꿈|이|었|을|까|요|?| 바|로| 왕|궁|에서| 왕|이| 잠|을| 자|는| 꿈|이|었|어|요|!\"\n",
      "\n",
      "|재|밌|고| 유|머|러|스|한| 농|담|이|었|나|요|?| 더| 들|려|드|릴|게| 있|으면| 언|제|든|지| 말|씀|해|주세요|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"안녕! 나는 재현이야. 내게 재밌는 농담을 들려줄래?\")],\n",
    "        \"language\": \"Korean\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
